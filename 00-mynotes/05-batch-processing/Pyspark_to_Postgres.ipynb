{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e85e2f1-25da-4da8-896a-7c6e9949ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6b5e0-4ca2-4fc1-be9e-442929cefb91",
   "metadata": {},
   "source": [
    "# Used this link for connecting all spark notebook to postgres instance.\n",
    "\n",
    "https://stackoverflow.com/questions/67062205/adding-postgressql-jdbc-driver-to-all-spark-notebook-using-docker-compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37512a1e-4eaa-43af-9f14-5075556c0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", \"/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.19.jar\") \\\n",
    "\t.master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41411585-af0f-41a6-924b-baf9ac667297",
   "metadata": {},
   "outputs": [],
   "source": [
    "studentDf = spark.createDataFrame([\n",
    "\tRow(id=1,name='vijay',marks=67),\n",
    "\tRow(id=2,name='Ajay',marks=88),\n",
    "\tRow(id=3,name='jay',marks=79),\n",
    "\tRow(id=4,name='vinay',marks=67),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e53f1e-43f8-4189-8331-7aa1446b3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| name|marks|\n",
      "+---+-----+-----+\n",
      "|  1|vijay|   67|\n",
      "|  2| Ajay|   88|\n",
      "|  3|  jay|   79|\n",
      "|  4|vinay|   67|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f7cbc64-56b4-41a6-b315-1262d196aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    " studentDf.select(\"id\",\"name\",\"marks\").write.format(\"jdbc\",)\\\n",
    "    .option(\"url\", \"jdbc:postgresql://pgdatabase:5432/ny_taxi\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"students\") \\\n",
    "    .option(\"user\", \"root\").option(\"password\", \"root\").save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315d3ba-bd7b-4980-bb1a-25c261b88a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0baf19e-c94d-484e-b8e0-1b0d785e353e",
   "metadata": {},
   "source": [
    "1. Import the required PySpark modules and create a PySpark session with the PostgreSQL JDBC driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2220db-989b-40cd-8796-479a009f2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", \"/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.19.jar\") \\\n",
    "\t.master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0555c-d58f-46fb-a18e-ea84b45eeef1",
   "metadata": {},
   "source": [
    "2. Import the required PySpark modules and create a PySpark session with the PostgreSQL JDBC driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ae22df1-34ac-4fc6-badb-cef90f4c18fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"jdbc:postgresql://pgdatabase:5432/ny_taxi\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b85e58-030c-481c-8632-3640522df3f5",
   "metadata": {},
   "source": [
    "3. Querying a PostgreSQL Table using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7893f52c-12dd-46ea-be35-f4ff0042cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"students\"\n",
    "\n",
    "df = spark.read.jdbc(url, table_name, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86028a6-fa76-460f-bcfe-b3a801d881b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| name|marks|\n",
      "+---+-----+-----+\n",
      "|  1|vijay|   67|\n",
      "|  2| Ajay|   88|\n",
      "|  3|  jay|   79|\n",
      "|  4|vinay|   67|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2300e755-139c-4653-9e8a-e321bc295767",
   "metadata": {},
   "source": [
    "4. Perform more complex queries using SQL\n",
    "   \n",
    "If you prefer to write SQL queries, you can register the DataFrame as a temporary table and then use SQL to query the data.\n",
    "\n",
    "Register the DataFrame as a temporary table and replace your_temp_table with a name for the temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16574801-290c-4663-aa24-d7bd6e0fa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"your_temp_table\")\n",
    "\n",
    "sql_query = \"SELECT * FROM your_temp_table WHERE marks > 68\"\n",
    "\n",
    "result_df = spark.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b458726-7aa7-4fa2-ad11-36ead05fc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|marks|\n",
      "+---+----+-----+\n",
      "|  2|Ajay|   88|\n",
      "|  3| jay|   79|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b940c705-ae42-4e24-9e42-6bc74aaf441f",
   "metadata": {},
   "source": [
    "5. Save the results back to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfd5e2c2-7480-402c-9c20-d9e7904d6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_name = \"students\"\n",
    "df.write.jdbc(url, result_table_name, mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95723fff-8da9-4ed3-b200-ef7fab69d7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
